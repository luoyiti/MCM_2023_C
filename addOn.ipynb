{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2d5f96",
   "metadata": {},
   "source": [
    "# 补充属性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69106618",
   "metadata": {},
   "source": [
    "## 语义难度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96fb517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d929c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>contest_number</th>\n",
       "      <th>word</th>\n",
       "      <th>number_of_reported_results</th>\n",
       "      <th>number_in_hard_mode</th>\n",
       "      <th>1_try</th>\n",
       "      <th>2_tries</th>\n",
       "      <th>3_tries</th>\n",
       "      <th>4_tries</th>\n",
       "      <th>5_tries</th>\n",
       "      <th>...</th>\n",
       "      <th>letter_freq_mean</th>\n",
       "      <th>letter_freq_min</th>\n",
       "      <th>positional_freq_mean</th>\n",
       "      <th>positional_freq_min</th>\n",
       "      <th>semantic_neighbors_count</th>\n",
       "      <th>semantic_density</th>\n",
       "      <th>Zipf-value</th>\n",
       "      <th>autoencoder_value</th>\n",
       "      <th>expected_attempts</th>\n",
       "      <th>semantic_difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022/12/31</td>\n",
       "      <td>560</td>\n",
       "      <td>manly</td>\n",
       "      <td>20380</td>\n",
       "      <td>1899</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052925</td>\n",
       "      <td>0.030641</td>\n",
       "      <td>0.091365</td>\n",
       "      <td>0.055710</td>\n",
       "      <td>0</td>\n",
       "      <td>0.350035</td>\n",
       "      <td>3.607804</td>\n",
       "      <td>-0.406941</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.574506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022/12/30</td>\n",
       "      <td>559</td>\n",
       "      <td>molar</td>\n",
       "      <td>21204</td>\n",
       "      <td>1973</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066072</td>\n",
       "      <td>0.030641</td>\n",
       "      <td>0.081337</td>\n",
       "      <td>0.025070</td>\n",
       "      <td>0</td>\n",
       "      <td>0.329278</td>\n",
       "      <td>2.723198</td>\n",
       "      <td>-0.631226</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0.711385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022/12/29</td>\n",
       "      <td>558</td>\n",
       "      <td>havoc</td>\n",
       "      <td>20001</td>\n",
       "      <td>1919</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051031</td>\n",
       "      <td>0.013928</td>\n",
       "      <td>0.049582</td>\n",
       "      <td>0.013928</td>\n",
       "      <td>1</td>\n",
       "      <td>0.402676</td>\n",
       "      <td>3.136932</td>\n",
       "      <td>-0.381673</td>\n",
       "      <td>4.44</td>\n",
       "      <td>-0.585251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022/12/28</td>\n",
       "      <td>557</td>\n",
       "      <td>impel</td>\n",
       "      <td>20160</td>\n",
       "      <td>1937</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057604</td>\n",
       "      <td>0.030641</td>\n",
       "      <td>0.054039</td>\n",
       "      <td>0.016713</td>\n",
       "      <td>0</td>\n",
       "      <td>0.334106</td>\n",
       "      <td>1.893894</td>\n",
       "      <td>-0.623626</td>\n",
       "      <td>4.21</td>\n",
       "      <td>0.679545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022/12/27</td>\n",
       "      <td>556</td>\n",
       "      <td>condo</td>\n",
       "      <td>20879</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053370</td>\n",
       "      <td>0.030084</td>\n",
       "      <td>0.070195</td>\n",
       "      <td>0.022284</td>\n",
       "      <td>0</td>\n",
       "      <td>0.408353</td>\n",
       "      <td>3.626288</td>\n",
       "      <td>-0.326174</td>\n",
       "      <td>4.51</td>\n",
       "      <td>0.189932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  contest_number   word  number_of_reported_results  \\\n",
       "0  2022/12/31             560  manly                       20380   \n",
       "1  2022/12/30             559  molar                       21204   \n",
       "2  2022/12/29             558  havoc                       20001   \n",
       "3  2022/12/28             557  impel                       20160   \n",
       "4  2022/12/27             556  condo                       20879   \n",
       "\n",
       "   number_in_hard_mode  1_try  2_tries  3_tries  4_tries  5_tries  ...  \\\n",
       "0                 1899      0        2       17       37       29  ...   \n",
       "1                 1973      0        4       21       38       26  ...   \n",
       "2                 1919      0        2       16       38       30  ...   \n",
       "3                 1937      0        3       21       40       25  ...   \n",
       "4                 2012      0        2       17       35       29  ...   \n",
       "\n",
       "   letter_freq_mean  letter_freq_min  positional_freq_mean  \\\n",
       "0          0.052925         0.030641              0.091365   \n",
       "1          0.066072         0.030641              0.081337   \n",
       "2          0.051031         0.013928              0.049582   \n",
       "3          0.057604         0.030641              0.054039   \n",
       "4          0.053370         0.030084              0.070195   \n",
       "\n",
       "   positional_freq_min  semantic_neighbors_count  semantic_density  \\\n",
       "0             0.055710                         0          0.350035   \n",
       "1             0.025070                         0          0.329278   \n",
       "2             0.013928                         1          0.402676   \n",
       "3             0.016713                         0          0.334106   \n",
       "4             0.022284                         0          0.408353   \n",
       "\n",
       "   Zipf-value  autoencoder_value  expected_attempts  semantic_difficulty  \n",
       "0    3.607804          -0.406941               4.42             0.574506  \n",
       "1    2.723198          -0.631226               4.20             0.711385  \n",
       "2    3.136932          -0.381673               4.44            -0.585251  \n",
       "3    1.893894          -0.623626               4.21             0.679545  \n",
       "4    3.626288          -0.326174               4.51             0.189932  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/mcm_processed_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e7147",
   "metadata": {},
   "source": [
    "## 语义领居数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "273bbf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(glove_path):\n",
    "    embeddings = {}\n",
    "    with open(glove_path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            # 后面是向量\n",
    "            vec = np.array(parts[1:], dtype=np.float32)\n",
    "            embeddings[word] = vec\n",
    "    print(f\"Loaded {len(embeddings)} word vectors.\")\n",
    "    return embeddings\n",
    "\n",
    "glove_path = \"data/glove.6B/glove.6B.300d.txt\"  # 替换为你的路径\n",
    "embeddings = load_glove_embeddings(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "626c946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordle_words = df['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b1820dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV words: []\n",
      "Have vectors for: 358 words\n"
     ]
    }
   ],
   "source": [
    "# 为每个单词取词向量\n",
    "def get_vector(word, embeddings):\n",
    "    w = word.lower()\n",
    "    if w in embeddings:\n",
    "        return embeddings[w]\n",
    "    else:\n",
    "        return None  # OOV 处理留给后面\n",
    "\n",
    "word_vectors = {}\n",
    "oov_words = []\n",
    "\n",
    "for w in wordle_words:\n",
    "    vec = get_vector(w, embeddings)\n",
    "    if vec is not None:\n",
    "        word_vectors[w] = vec\n",
    "    else:\n",
    "        oov_words.append(w)\n",
    "\n",
    "print(\"OOV words:\", oov_words)\n",
    "print(\"Have vectors for:\", len(word_vectors), \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70f8117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义余弦相似度\n",
    "def cosine_sim(vec1, vec2):\n",
    "    num = np.dot(vec1, vec2)\n",
    "    denom = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    if denom == 0.0:\n",
    "        return 0.0\n",
    "    return num / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52360bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.5\n",
    "\n",
    "words = list(word_vectors.keys())\n",
    "vecs = [word_vectors[w] for w in words if len(w) == 5]\n",
    "\n",
    "glove_words = list(embeddings.keys())\n",
    "glove_vecs = [embeddings[w.lower()] for w in glove_words if len(w) == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0db422c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16a25541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 358/358 [01:14<00:00,  4.81it/s]\n"
     ]
    }
   ],
   "source": [
    "semantic_neighbors_count = {}\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, w in enumerate(tqdm(words)):\n",
    "    v_w = vecs[i]\n",
    "    count = 0\n",
    "    for word, glove_vecs in embeddings.items():\n",
    "        if len(word) != 5:\n",
    "            continue\n",
    "        if w == word:\n",
    "            continue\n",
    "        sim = cosine_sim(v_w, glove_vecs)\n",
    "        if sim >= tau:\n",
    "            count += 1\n",
    "    semantic_neighbors_count[w] = count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75e988d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"semantic_neighbors_count\"] = df[\"word\"].map(semantic_neighbors_count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ec3c56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 358/358 [01:03<00:00,  5.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from heapq import nlargest\n",
    "\n",
    "k = 10\n",
    "semantic_density = {}\n",
    "\n",
    "for i, w in enumerate(tqdm(words)):\n",
    "    v_w = vecs[i]\n",
    "    sims = []\n",
    "    for word, glove_vecs in embeddings.items():\n",
    "        if len(word) != 5:\n",
    "            continue\n",
    "        if w == word:\n",
    "            continue\n",
    "        sims.append(cosine_sim(v_w, glove_vecs))\n",
    "    topk = nlargest(k, sims)\n",
    "    semantic_density[w] = float(sum(topk)) / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1a54a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['semantic_density'] = df['word'].map(semantic_density.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2001a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_log = np.log1p(df['semantic_neighbors_count'])\n",
    "neighbors_z = (neighbors_log - neighbors_log.mean()) / neighbors_log.std()\n",
    "density_z = (df['semantic_density'] - df['semantic_density'].mean()) / df['semantic_density'].std()\n",
    "df['semantic_difficulty'] = -(0.5 * neighbors_z + 0.5 * density_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99624575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/mcm_processed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65791dd0",
   "metadata": {},
   "source": [
    "## 词频难度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0874053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/mcm_processed_data.csv.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f6dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_excel(\"data/SUBTLEX-US frequency list with PoS and Zipf information.xlsx\")\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede59bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 df 的列是 'word'，dt 的列是 'Word' 和 'Zipf-value'\n",
    "df = df.merge(\n",
    "    dt[['Word', 'Zipf-value']],\n",
    "    left_on='word',\n",
    "    right_on='Word',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 如果不需要保留 'Word' 这一列，可以删掉\n",
    "df = df.drop(columns=['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d8fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_log = np.log1p(df['semantic_neighbors_count'])\n",
    "neighbors_z = (neighbors_log - neighbors_log.mean()) / neighbors_log.std()\n",
    "density_z = (df['semantic_density'] - df['semantic_density'].mean()) / df['semantic_density'].std()\n",
    "df['semantic_difficulty'] = -(0.5 * neighbors_z + 0.5 * density_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754aa232",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c3fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"data/Copydata_with_features.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7de468",
   "metadata": {},
   "source": [
    "## 汉明距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d873b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('data/allowed.txt', 'r') as f:\n",
    "    allowed = f.read().splitlines()\n",
    "allowed = [word.lower().strip() for word in allowed]\n",
    "print(allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d45b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(w1, w2):\n",
    "    # 定义汉明距离函数\n",
    "    return sum(c1 != c2 for c1, c2 in zip(w1, w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd4afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = {}\n",
    "\n",
    "for w in df['word'].tolist():\n",
    "    cnt = 0\n",
    "    for v in allowed:\n",
    "        if w != v and hamming_distance(w, v) == 1:\n",
    "            cnt += 1\n",
    "\n",
    "    neighbors[w] = cnt\n",
    "\n",
    "print(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43337503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 neighbors 字典映射到 df，生成一列\n",
    "df['hamming_neighbors'] = df['word'].map(neighbors)\n",
    "df.to_csv('data/mcm_processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a2b51c",
   "metadata": {},
   "source": [
    "## Lasso回归分析 - 变量重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e543778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV\n",
    "df = pd.read_csv(\"data/mcm_processed_data.csv\")\n",
    "\n",
    "# 定义需要排除的列\n",
    "exclude_cols = ['date', 'contest_number', 'word', 'number_of_reported_results', 'number_in_hard_mode',\n",
    "                '1_try', '2_tries', '3_tries', '4_tries', '5_tries', '6_tries', '7_or_more_tries_x', \n",
    "                'sum', 'autoencoder_value', 'expected_attempts']\n",
    "\n",
    "# 选择特征列\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "print(\"特征变量:\", feature_cols)\n",
    "print(f\"\\n共 {len(feature_cols)} 个特征变量\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059adbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 准备数据\n",
    "X = df[feature_cols].copy()\n",
    "y = df['autoencoder_value'].copy()\n",
    "\n",
    "# 检查目标变量\n",
    "print(\"目标变量统计:\")\n",
    "print(y.describe())\n",
    "print(f\"\\n缺失值数量: {y.isna().sum()}\")\n",
    "\n",
    "# 处理缺失值\n",
    "X = X.fillna(X.median())\n",
    "y = y.fillna(y.median())\n",
    "\n",
    "# 标准化特征\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\n特征矩阵形状: {X_scaled.shape}\")\n",
    "print(f\"目标变量形状: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbdc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用交叉验证选择最优的alpha参数\n",
    "alphas_to_try = np.logspace(-8, -1, 100)\n",
    "lasso_cv = LassoCV(alphas=alphas_to_try, cv=5, random_state=42, max_iter=10000)\n",
    "lasso_cv.fit(X_scaled, y)\n",
    "\n",
    "print(f\"交叉验证选择的最优 alpha: {lasso_cv.alpha_:.8f}\")\n",
    "print(f\"R² 得分 (最优alpha): {lasso_cv.score(X_scaled, y):.4f}\")\n",
    "\n",
    "# 测试不同alpha值的效果\n",
    "print(\"\\n不同alpha值的模型效果:\")\n",
    "for alpha in [0.0001, 0.001, 0.005, 0.007, 0.008, 0.01, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1]:\n",
    "    lasso_test = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso_test.fit(X_scaled, y)\n",
    "    n_nonzero = (lasso_test.coef_ != 0).sum()\n",
    "    r2 = lasso_test.score(X_scaled, y)\n",
    "    print(f\"  alpha={alpha:.4f}: R²={r2:.4f}, 非零系数={n_nonzero}\")\n",
    "\n",
    "# 选择一个能保留足够特征的alpha进行分析\n",
    "analysis_alpha = lasso_cv.alpha_\n",
    "lasso_final = Lasso(alpha=analysis_alpha, max_iter=10000)\n",
    "lasso_final.fit(X_scaled, y)\n",
    "print(f\"\\n最终选择 alpha={analysis_alpha} 进行变量重要性分析\")\n",
    "print(f\"R² 得分: {lasso_final.score(X_scaled, y):.4f}\")\n",
    "print(f\"非零系数数量: {(lasso_final.coef_ != 0).sum()}/{len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537533d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取特征系数并排序\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'coefficient': lasso_final.coef_,\n",
    "    'abs_coefficient': np.abs(lasso_final.coef_)\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "# 显示所有特征的重要性\n",
    "print(\"=\" * 60)\n",
    "print(f\"Lasso回归特征重要性排序 (alpha={analysis_alpha})\")\n",
    "print(\"=\" * 60)\n",
    "for i, row in coef_df.iterrows():\n",
    "    status = \"✓\" if row['coefficient'] != 0 else \"✗\"\n",
    "    print(f\"{status} {row['feature']:30s} : {row['coefficient']:+.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"非零系数特征数: {(coef_df['coefficient'] != 0).sum()} / {len(feature_cols)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b4b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化特征重要性\n",
    "plt.rcParams['font.family'] = 'Heiti TC'  # 替换为你选择的字体\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# 只显示非零系数的特征\n",
    "nonzero_df = coef_df[coef_df['coefficient'] != 0].copy()\n",
    "\n",
    "# 图1: 按系数值排序的条形图\n",
    "ax1 = axes[0]\n",
    "colors = ['green' if x > 0 else 'red' for x in nonzero_df['coefficient']]\n",
    "bars = ax1.barh(range(len(nonzero_df)), nonzero_df['coefficient'], color=colors, alpha=0.7)\n",
    "ax1.set_yticks(range(len(nonzero_df)))\n",
    "ax1.set_yticklabels(nonzero_df['feature'])\n",
    "ax1.set_xlabel('Lasso Coefficient')\n",
    "ax1.set_title('Lasso回归系数 (非零特征)')\n",
    "ax1.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# 图2: 按绝对值排序的条形图\n",
    "ax2 = axes[1]\n",
    "sorted_by_abs = nonzero_df.sort_values('abs_coefficient', ascending=True)\n",
    "colors2 = ['green' if x > 0 else 'red' for x in sorted_by_abs['coefficient']]\n",
    "ax2.barh(range(len(sorted_by_abs)), sorted_by_abs['abs_coefficient'], color=colors2, alpha=0.7)\n",
    "ax2.set_yticks(range(len(sorted_by_abs)))\n",
    "ax2.set_yticklabels(sorted_by_abs['feature'])\n",
    "ax2.set_xlabel('|Lasso Coefficient|')\n",
    "ax2.set_title('特征重要性 (按绝对值排序)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/lasso_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n图例: 绿色=正相关, 红色=负相关\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同alpha值下的系数路径图\n",
    "alphas = np.logspace(-5, -1, 50)\n",
    "coefs = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X_scaled, y)\n",
    "    coefs.append(lasso.coef_)\n",
    "\n",
    "coefs = np.array(coefs)\n",
    "\n",
    "# 绘制系数路径\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, feature in enumerate(feature_cols):\n",
    "    plt.plot(alphas, coefs[:, i], label=feature, linewidth=1.5)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Alpha (正则化强度)')\n",
    "plt.ylabel('系数值')\n",
    "plt.title('Lasso回归系数路径图')\n",
    "plt.axvline(x=analysis_alpha, color='black', linestyle='--', label=f'选定alpha={analysis_alpha}')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/lasso_coefficient_path.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff599c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算所有构建指标的皮尔森相关性\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 选择数值特征列（排除非数值列和ID列）\n",
    "exclude_cols = ['date', 'contest_number', 'word', 'number_of_reported_results', 'number_in_hard_mode',\n",
    "                '1_try', '2_tries', '3_tries', '4_tries', '5_tries', '6_tries', '7_or_more_tries_x', \n",
    "                'sum']\n",
    "numeric_features = [col for col in df.columns if col not in exclude_cols and df[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "print(f\"用于相关性分析的数值特征 ({len(numeric_features)} 个):\")\n",
    "for i, col in enumerate(numeric_features, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# 计算相关性矩阵\n",
    "correlation_matrix = df[numeric_features].corr()\n",
    "\n",
    "print(f\"\\n相关性矩阵形状: {correlation_matrix.shape}\")\n",
    "print(\"\\n相关性矩阵前5x5:\")\n",
    "print(correlation_matrix.iloc[:5, :5].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cdec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建相关性热力图\n",
    "plt.figure(figsize=(16, 14))\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 创建热力图\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # 只显示下三角\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={\"shrink\": 0.8},\n",
    "            annot_kws={'size': 8})\n",
    "\n",
    "plt.title('所有构建指标间的皮尔森相关性矩阵', fontsize=16, pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存图片\n",
    "plt.savefig('data/correlation_heatmap.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"相关性热力图已保存到: data/correlation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf4ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出高相关性特征对\n",
    "print(\"=\" * 80)\n",
    "print(\"高相关性特征对分析 (|r| ≥ 0.5)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) >= 0.5:  # 阈值设为0.5\n",
    "            feature1 = correlation_matrix.columns[i]\n",
    "            feature2 = correlation_matrix.columns[j]\n",
    "            high_corr_pairs.append((feature1, feature2, corr_value))\n",
    "\n",
    "# 按相关性绝对值排序\n",
    "high_corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "print(f\"找到 {len(high_corr_pairs)} 对高相关性特征:\\n\")\n",
    "\n",
    "for feature1, feature2, corr_value in high_corr_pairs:\n",
    "    direction = \"正相关\" if corr_value > 0 else \"负相关\"\n",
    "    print(f\"{feature1:30s} ↔ {feature2:30s}: {corr_value:+.3f} ({direction})\")\n",
    "\n",
    "# 特别关注语义相关指标\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"语义相关指标与其他特征的相关性:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "semantic_features = ['semantic_neighbors_count', 'semantic_density', 'semantic_difficulty', 'Zipf-value']\n",
    "\n",
    "for feature in semantic_features:\n",
    "    if feature in correlation_matrix.columns:\n",
    "        correlations = correlation_matrix[feature].sort_values(key=abs, ascending=False)\n",
    "        print(f\"\\n{feature} 的相关性排序 (前10个):\")\n",
    "        for other_feature, corr_value in correlations.head(10).items():\n",
    "            if other_feature != feature:\n",
    "                direction = \"正相关\" if corr_value > 0 else \"负相关\"\n",
    "                print(f\"  {other_feature:30s}: {corr_value:+.3f} ({direction})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37334845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存相关性分析结果\n",
    "import os\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# 保存相关性矩阵到Excel\n",
    "correlation_matrix.to_excel('data/correlation_matrix.xlsx')\n",
    "print(\"相关性矩阵已保存到: data/correlation_matrix.xlsx\")\n",
    "\n",
    "# 保存高相关性对到文本文件\n",
    "with open('data/high_correlation_pairs.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"高相关性特征对分析 (|r| ≥ 0.5)\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"找到 {len(high_corr_pairs)} 对高相关性特征:\\n\\n\")\n",
    "    \n",
    "    for feature1, feature2, corr_value in high_corr_pairs:\n",
    "        direction = \"正相关\" if corr_value > 0 else \"负相关\"\n",
    "        f.write(f\"{feature1:30s} ↔ {feature2:30s}: {corr_value:+.3f} ({direction})\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    f.write(\"语义相关指标与其他特征的相关性:\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    for feature in semantic_features:\n",
    "        if feature in correlation_matrix.columns:\n",
    "            correlations = correlation_matrix[feature].sort_values(key=abs, ascending=False)\n",
    "            f.write(f\"\\n{feature} 的相关性排序 (前10个):\\n\")\n",
    "            for other_feature, corr_value in correlations.head(10).items():\n",
    "                if other_feature != feature:\n",
    "                    direction = \"正相关\" if corr_value > 0 else \"负相关\"\n",
    "                    f.write(f\"  {other_feature:30s}: {corr_value:+.3f} ({direction})\\n\")\n",
    "\n",
    "print(\"高相关性分析结果已保存到: data/high_correlation_pairs.txt\")\n",
    "\n",
    "# 统计信息\n",
    "print(f\"\\n相关性分析总结:\")\n",
    "print(f\"- 总特征数: {len(numeric_features)}\")\n",
    "print(f\"- 相关性矩阵大小: {correlation_matrix.shape}\")\n",
    "print(f\"- 高相关性特征对数 (|r| ≥ 0.5): {len(high_corr_pairs)}\")\n",
    "print(f\"- 极高相关性特征对数 (|r| ≥ 0.8): {sum(1 for _, _, r in high_corr_pairs if abs(r) >= 0.8)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10793e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汇总表格\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Lasso回归分析结果汇总\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_df = coef_df[['feature', 'coefficient', 'abs_coefficient']].copy()\n",
    "summary_df['rank'] = range(1, len(summary_df) + 1)\n",
    "summary_df['selected'] = summary_df['coefficient'] != 0\n",
    "summary_df = summary_df[['rank', 'feature', 'coefficient', 'abs_coefficient', 'selected']]\n",
    "summary_df.columns = ['排名', '特征变量', '系数', '系数绝对值', '被选择']\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# 保存结果\n",
    "summary_df.to_excel('data/lasso_feature_importance.xlsx', index=False)\n",
    "print(\"\\n结果已保存到 data/lasso_feature_importance.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff88e8f2",
   "metadata": {},
   "source": [
    "## Lasso回归分析结论\n",
    "\n",
    "### 重要发现\n",
    "\n",
    "**最重要的正相关变量** (系数为正，autoencoder_value增大时变量值增大):\n",
    "1. **semantic_density** (0.0325): 语义密度越高，autoencoder_value越大\n",
    "2. **positional_freq_min** (0.0307): 最小位置字母频率越高越重要\n",
    "3. **has_common_prefix** (0.0259): 有常见前缀的单词\n",
    "4. **ends_with_vowel** (0.0159): 以元音结尾\n",
    "5. **starts_with_vowel** (0.0142): 以元音开头\n",
    "\n",
    "**最重要的负相关变量** (系数为负):\n",
    "1. **semantic_neighbors_count** (-0.0347): 语义邻居数量越多，autoencoder_value越小\n",
    "2. **num_vowels** (-0.0248): 元音数量越多越低\n",
    "3. **contains_y** (-0.0199): 包含字母Y\n",
    "4. **hamming_neighbors** (-0.0154): 汉明距离邻居数\n",
    "5. **num_multiple_letters** (-0.0154): 重复字母数量\n",
    "\n",
    "**被Lasso筛除的变量** (系数为0，相对不重要):\n",
    "- unique_letters, has_repeats, num_rare_letters, scrabble_score\n",
    "- letter_entropy, position_rarity, keyboard_distance\n",
    "- has_common_suffix, has_double_letter, letter_freq_mean\n",
    "\n",
    "### 注意事项\n",
    "- 整体模型R²较低 (~0.03)，说明autoencoder_value的变异主要由其他未包含的因素解释\n",
    "- Lasso回归的特征选择功能帮助识别了16个相对重要的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0e634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
