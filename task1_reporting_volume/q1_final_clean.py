"""
Q1ï¼šæ—¶é—´åºåˆ—é›†æˆé¢„æµ‹ï¼ˆç²¾ç®€ç¨³å¥ç‰ˆï¼‰

æ ¸å¿ƒç‚¹ï¼š
1) æ¯æŠ˜ç‹¬ç«‹ PELT å˜ç‚¹ï¼ˆé˜²æ³„éœ²ï¼‰
2) log1p æ–¹å·®ç¨³å®š
3) åˆ†æ®µ SARIMA + æ»šåŠ¨ CV
4) åˆå¥=å‡å€¼åŠ æƒ + æ–¹å·®åˆå¹¶ï¼ˆæ­£ç¡®åˆæˆ CIï¼‰
5) Duan smearing å›å˜æ¢çº å
6) è¾“å‡ºä¸ Prophet åŒå£å¾„çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆRMSE/MASE/Coverage/Winkler/Pinballï¼‰
"""

from __future__ import annotations

import argparse
import pickle
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from statsmodels.tsa.statespace.sarimax import SARIMAX
import warnings
import holidays

from viz_report import (
    diagnostic_plots,
    generate_diagnostic_report,
    plot_weekday_effects,
    plot_changepoint_summary,
    plot_factor_importance,
)

warnings.filterwarnings('ignore')

try:
    import ruptures as rpt
except ImportError:
    raise ImportError("éœ€è¦å®‰è£… ruptures: pip install ruptures")


# ======================
# è¯„ä¼°ä¸åˆå¥å°å·¥å…·
# ======================

def compute_mase(y_true: np.ndarray, y_pred: np.ndarray, y_train: np.ndarray, season: int = 7) -> float:
    """MASEï¼ˆé»˜è®¤ä»¥å­£èŠ‚æœ´ç´  s=7 ä½œä¸ºåŸºçº¿ï¼‰"""
    if len(y_train) <= season:
        return np.nan
    denom = np.mean(np.abs(y_train[season:] - y_train[:-season]))
    if denom == 0:
        return np.nan
    return float(np.mean(np.abs(y_true - y_pred)) / denom)

def compute_winkler_score(y_true: np.ndarray, y_lower: np.ndarray, y_upper: np.ndarray, alpha: float = 0.10) -> float:
    """Winkler@1-alpha åŒºé—´è¯„åˆ†ï¼ˆè¶Šå°è¶Šå¥½ï¼›alpha=0.10 å¯¹åº” 90% CIï¼‰"""
    width = y_upper - y_lower
    under = y_true < y_lower
    over  = y_true > y_upper
    penalty = (y_lower - y_true) * under + (y_true - y_upper) * over
    return float(np.mean(width + (2.0/alpha) * penalty))

def compute_pinball_loss_point(y_true: np.ndarray, y_pred: np.ndarray, taus=(0.1, 0.5, 0.9)) -> dict:
    """ç®€åŒ–ç‰ˆ pinballï¼šç”¨ç‚¹é¢„æµ‹è¿‘ä¼¼å¯¹åº”åˆ†ä½ï¼ˆç”¨äºæ¨ªæ¯”å³å¯ï¼‰"""
    e = y_true - y_pred
    out = {}
    for t in taus:
        out[f"Pin@{int(t*100)}"] = float(np.mean(np.maximum(t*e, (t-1)*e)))
    return out

def combine_mean_se(means_log: List[np.ndarray], ses_log: List[np.ndarray], weights: np.ndarray):
    """åˆå¥ï¼šå‡å€¼åŠ æƒ + æ–¹å·®åˆå¹¶ï¼ˆç‹¬ç«‹è¿‘ä¼¼ï¼›å…¨éƒ¨åœ¨ log ç©ºé—´ï¼‰"""
    w = np.asarray(weights)
    w = w / np.sum(w)
    mu = np.average(np.stack(means_log), axis=0, weights=w)
    var = np.sum((w[:, None]**2) * (np.stack(ses_log)**2), axis=0)
    return mu, np.sqrt(var), w

def duan_smearing(pred_log: np.ndarray, residuals_log: np.ndarray) -> np.ndarray:
    """Duan's smearing çº åå›åŸå°ºåº¦"""
    if residuals_log is None or len(residuals_log) == 0:
        return np.expm1(pred_log)
    k = float(np.mean(np.exp(residuals_log)))
    return np.exp(pred_log) * k - 1.0


# ======================
# æ•°æ®&ç‰¹å¾
# ======================

def load_data(path: Path) -> pd.DataFrame:
    """
    åŠ è½½æ•°æ®ï¼Œæ”¯æŒ Excel å’Œ CSV æ ¼å¼
    ä¼˜å…ˆä½¿ç”¨ CSVï¼ˆåŒ…å«çœŸå®æ•°æ®ï¼‰ï¼ŒExcel æ–‡ä»¶æ˜¯å½’ä¸€åŒ–çš„ç™¾åˆ†æ¯”æ•°æ®
    """
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹
    if path.suffix == '.csv':
        df = pd.read_csv(path)
        # CSV çš„åˆ—åæ ¼å¼
        if 'date' in df.columns:
            df['Date'] = pd.to_datetime(df['date'])
        # CSV å·²ç»æœ‰çœŸå®çš„æŠ¥å‘Šäººæ•°
        if 'number_of_reported_results' in df.columns:
            df['Number of  reported results'] = df['number_of_reported_results']
    else:
        # Excel æ ¼å¼ï¼ˆæ³¨æ„ï¼šè¿™æ˜¯å½’ä¸€åŒ–çš„ç™¾åˆ†æ¯”æ•°æ®ï¼ï¼‰
        df = pd.read_excel(path, header=0)
        df.columns = df.columns.str.strip()
        
        if 'timestamp' in df.columns:
            df['Date'] = pd.to_datetime(df['timestamp'])
        elif 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'])
        else:
            raise KeyError("æ‰¾ä¸åˆ°æ—¥æœŸåˆ—ï¼ˆ'timestamp' æˆ– 'Date'ï¼‰")
        
        # è®¡ç®—æ€»æŠ¥å‘Šäººæ•°ï¼ˆExcel æ˜¯ç™¾åˆ†æ¯”ï¼Œéœ€è¦æ±‚å’Œï¼‰
        try_columns = ['1 try', '2 tries', '3 tries', '4 tries', '5 tries', '6 tries', '7 or more tries (X)']
        df['Number of  reported results'] = df[try_columns].sum(axis=1)
    
    df = df.loc[:, ~df.columns.str.contains('^Unnamed', case=False)]
    return df.sort_values('Date').reset_index(drop=True)

def detect_changepoint(ts: pd.Series) -> int:
    """PELT å˜ç‚¹ï¼ˆåœ¨åŸå§‹å°ºåº¦åš log1p å†æ£€æµ‹ï¼‰"""
    algo = rpt.Pelt(model="l2", min_size=30).fit(np.log(ts.values + 1))
    result = algo.predict(pen=3)
    return result[0] if len(result) > 1 else len(ts) // 2

def add_features(df: pd.DataFrame, changepoint_idx: int, ts: pd.Series) -> pd.DataFrame:
    exog = pd.DataFrame(index=ts.index)
    exog['regime'] = 0.0
    if changepoint_idx > 0:
        exog.loc[exog.index[changepoint_idx:], 'regime'] = 1.0
    
    dates = df['Date'].values if 'Date' in df.columns else ts.index
    weekday = pd.DatetimeIndex(dates).dayofweek
    exog['is_weekend'] = weekday.isin([5, 6]).astype(float)
    
    us_holidays = holidays.US(years=[2022, 2023])
    exog['is_holiday'] = pd.DatetimeIndex(dates).map(lambda x: float(x in us_holidays))
    
    return exog


def generate_explanation_report(ts_raw: pd.Series, exog: pd.DataFrame, 
                               models: Dict, cp_idx: int, 
                               output_dir: Path) -> Dict:
    """
    ç”Ÿæˆè¯¦ç»†çš„å¯è§£é‡Šæ€§åˆ†ææŠ¥å‘Š
    """
    from statsmodels.tsa.seasonal import STL
    
    # 1. å˜ç‚¹åˆ†æ
    cp_date = ts_raw.index[cp_idx]
    pre_cp = ts_raw.iloc[:cp_idx]
    post_cp = ts_raw.iloc[cp_idx:]
    cp_drop_abs = pre_cp.mean() - post_cp.mean()
    cp_drop_pct = cp_drop_abs / pre_cp.mean() * 100
    
    # 2. å‘¨æœ«æ•ˆåº”
    weekday_mask = exog['is_weekend'] == 0
    weekend_mask = exog['is_weekend'] == 1
    weekday_avg = ts_raw[weekday_mask].mean()
    weekend_avg = ts_raw[weekend_mask].mean()
    weekend_effect_abs = weekend_avg - weekday_avg
    weekend_effect_pct = weekend_effect_abs / weekday_avg * 100
    
    # 3. èŠ‚å‡æ—¥æ•ˆåº”
    holiday_mask = exog['is_holiday'] == 1
    non_holiday_mask = exog['is_holiday'] == 0
    if holiday_mask.sum() > 0:
        holiday_avg = ts_raw[holiday_mask].mean()
        non_holiday_avg = ts_raw[non_holiday_mask].mean()
        holiday_effect_pct = (holiday_avg - non_holiday_avg) / non_holiday_avg * 100
    else:
        holiday_avg = 0
        holiday_effect_pct = 0
    
    # 4. è¶‹åŠ¿åˆ†è§£ï¼ˆSTLï¼‰
    try:
        stl = STL(ts_raw, seasonal=7, period=7)
        stl_result = stl.fit()
        trend = stl_result.trend
        seasonal = stl_result.seasonal
        
        # è®¡ç®—è¶‹åŠ¿æ–œç‡ï¼ˆæœ€è¿‘30å¤© vs å‰30å¤©ï¼‰
        recent_trend = trend.iloc[-30:].mean()
        prev_trend = trend.iloc[-60:-30].mean()
        trend_change = recent_trend - prev_trend
        trend_change_pct = trend_change / prev_trend * 100 if prev_trend != 0 else 0
        
        seasonal_strength = seasonal.std()
        
    except Exception as e:
        print(f"    [è­¦å‘Š] STLåˆ†è§£å¤±è´¥: {e}")
        trend_change = 0
        trend_change_pct = 0
        seasonal_strength = 0
    
    # 5. æ¨¡å‹æ€§èƒ½
    best = models['best']
    cv_metrics = {
        'model_name': best['name'],
        'cv_rmse': best.get('cv_rmse', 0),
        'cv_mase': best.get('cv_mase', 'N/A'),
    }
    
    # 6. æ³¢åŠ¨æ€§åˆ†æ
    overall_std = ts_raw.std()
    recent_std = ts_raw.iloc[-30:].std()
    volatility_change_pct = (recent_std - overall_std) / overall_std * 100
    
    # ç”ŸæˆæŠ¥å‘Šæ–‡æœ¬
    report = f"""
{'='*80}
                        WordleæŠ¥å‘Šäººæ•°è§£é‡Šæ€§åˆ†æ
{'='*80}

æ•°æ®æ¦‚å†µ
--------
è§‚æµ‹æœŸé—´: {ts_raw.index[0].date()} è‡³ {ts_raw.index[-1].date()} (å…±{len(ts_raw)}å¤©)
æ€»ä½“å‡å€¼: {ts_raw.mean():,.0f} äºº/å¤©
æ€»ä½“æ ‡å‡†å·®: {overall_std:,.0f} äºº
å˜å¼‚ç³»æ•°: {overall_std/ts_raw.mean()*100:.1f}%

{'='*80}
1. ç»“æ„æ€§å˜åŒ–åˆ†æï¼ˆChangepoint Effectï¼‰
{'='*80}

å˜ç‚¹æ£€æµ‹ç»“æœ:
  å˜ç‚¹æ—¥æœŸ: {cp_date.date()}
  å˜ç‚¹ä½ç½®: ç¬¬{cp_idx}å¤© / {len(ts_raw)}å¤© ({cp_idx/len(ts_raw)*100:.1f}%)

å˜ç‚¹å‰åå¯¹æ¯”:
  å˜ç‚¹å‰å‡å€¼: {pre_cp.mean():,.0f} äºº/å¤© (n={len(pre_cp)}å¤©)
  å˜ç‚¹åå‡å€¼: {post_cp.mean():,.0f} äºº/å¤© (n={len(post_cp)}å¤©)
  ç»å¯¹ä¸‹é™: {cp_drop_abs:,.0f} äºº/å¤©
  ç›¸å¯¹ä¸‹é™: {cp_drop_pct:.1f}%

å¯èƒ½è§£é‡Š:
  â€¢ æ–°é²œæ„Ÿæ¶ˆé€€: æ¸¸æˆå‘å¸ƒåˆæœŸçƒ­åº¦è‡ªç„¶å›è½
  â€¢ ç”¨æˆ·æµå¤±: éƒ¨åˆ†ç©å®¶å¤±å»å…´è¶£æˆ–è½¬å‘å…¶ä»–æ¸¸æˆ
  â€¢ ç¤¾äº¤ä¼ æ’­å‡å¼±: æœ‹å‹åœˆæ™’æˆç»©çš„é£æ½®å‡é€€
  â€¢ éš¾åº¦æ„ŸçŸ¥: éšç€å•è¯åº“è¢«æ¶ˆè€—ï¼Œç©å®¶å¯èƒ½è§‰å¾—éš¾åº¦æå‡

{'='*80}
2. å‘¨æœŸæ€§æ¨¡å¼åˆ†æï¼ˆCyclical Patternsï¼‰
{'='*80}

å‘¨æœ«æ•ˆåº”:
  å·¥ä½œæ—¥(å‘¨ä¸€è‡³å‘¨äº”)å‡å€¼: {weekday_avg:,.0f} äºº/å¤©
  å‘¨æœ«(å‘¨å…­å‘¨æ—¥)å‡å€¼: {weekend_avg:,.0f} äºº/å¤©
  ç»å¯¹å·®å¼‚: {weekend_effect_abs:,.0f} äºº/å¤©
  ç›¸å¯¹å·®å¼‚: {weekend_effect_pct:+.1f}%

"""
    
    if weekend_effect_pct < -3:
        report += "  è§£é‡Š: å·¥ä½œæ—¥å‚ä¸åº¦æ›´é«˜ï¼Œå¯èƒ½å› ä¸ºåŠå…¬å®¤æ–‡åŒ–ã€ç¤¾äº¤åª’ä½“åˆ†äº«æˆ–ä¼‘æ¯æ—¶é—´çš„å¨±ä¹éœ€æ±‚\n"
    elif weekend_effect_pct > 3:
        report += "  è§£é‡Š: å‘¨æœ«ç©å®¶æ›´æ´»è·ƒï¼Œæœ‰æ›´å¤šé—²æš‡æ—¶é—´ç©æ¸¸æˆå¹¶åˆ†äº«æˆç»©\n"
    else:
        report += "  è§£é‡Š: å‘¨æœ«æ•ˆåº”ä¸æ˜æ˜¾ï¼Œç©å®¶è¡Œä¸ºè¾ƒä¸ºç¨³å®š\n"
    
    report += f"""
èŠ‚å‡æ—¥æ•ˆåº”:
  éèŠ‚å‡æ—¥å‡å€¼: {non_holiday_avg:,.0f} äºº/å¤©
  èŠ‚å‡æ—¥å‡å€¼: {holiday_avg:,.0f} äºº/å¤© (n={holiday_mask.sum()}å¤©)
  ç›¸å¯¹å·®å¼‚: {holiday_effect_pct:+.1f}%

å­£èŠ‚æ€§å¼ºåº¦:
  STLå­£èŠ‚é¡¹æ ‡å‡†å·®: {seasonal_strength:.0f} äºº
  è§£é‡Š: {'å‘¨å†…æ³¢åŠ¨è¾ƒå¼º' if seasonal_strength > overall_std*0.2 else 'å‘¨å†…æ³¢åŠ¨è¾ƒå¼±'}

{'='*80}
3. è¶‹åŠ¿åˆ†æï¼ˆTrend Dynamicsï¼‰
{'='*80}

é•¿æœŸè¶‹åŠ¿:
  å‰30å¤©è¶‹åŠ¿å‡å€¼: {prev_trend:,.0f} äºº/å¤©
  æœ€è¿‘30å¤©è¶‹åŠ¿å‡å€¼: {recent_trend:,.0f} äºº/å¤©
  è¶‹åŠ¿å˜åŒ–: {trend_change:,.0f} äºº/å¤©
  å˜åŒ–ç‡: {trend_change_pct:+.1f}%

"""
    
    if trend_change_pct < -5:
        report += "  è§£é‡Š: æŒç»­ä¸‹é™è¶‹åŠ¿ï¼Œç©å®¶ç¾¤ä½“ä»åœ¨æµå¤±\n"
    elif trend_change_pct > 5:
        report += "  è§£é‡Š: è¶‹åŠ¿å‘ä¸Šï¼Œå¯èƒ½æœ‰æ–°çš„æ¨å¹¿æ´»åŠ¨æˆ–ç—…æ¯’å¼ä¼ æ’­\n"
    else:
        report += "  è§£é‡Š: è¶‹åŠ¿è¶‹äºç¨³å®šï¼Œæ ¸å¿ƒç©å®¶ç¾¤ä½“ç›¸å¯¹å›ºå®š\n"
    
    report += f"""
æ³¢åŠ¨æ€§å˜åŒ–:
  æ•´ä½“æ ‡å‡†å·®: {overall_std:,.0f} äºº
  æœ€è¿‘30å¤©æ ‡å‡†å·®: {recent_std:,.0f} äºº
  æ³¢åŠ¨æ€§å˜åŒ–: {volatility_change_pct:+.1f}%
  è§£é‡Š: {'æ³¢åŠ¨æ€§å¢åŠ ï¼Œé¢„æµ‹ä¸ç¡®å®šæ€§ä¸Šå‡' if volatility_change_pct > 10 else 'æ³¢åŠ¨æ€§ç¨³å®š'}

{'='*80}
4. æ¨¡å‹éªŒè¯ï¼ˆModel Performanceï¼‰
{'='*80}

æœ€ä¼˜æ¨¡å‹: {cv_metrics['model_name']}
äº¤å‰éªŒè¯RMSE: {cv_metrics['cv_rmse']:,.0f} äºº
äº¤å‰éªŒè¯MASE: {cv_metrics['cv_mase']}

æ¨¡å‹å¯é æ€§:
  â€¢ ä½¿ç”¨æ»šåŠ¨äº¤å‰éªŒè¯é˜²æ­¢è¿‡æ‹Ÿåˆ
  â€¢ æ¯æŠ˜ç‹¬ç«‹æ£€æµ‹å˜ç‚¹é˜²æ­¢æ•°æ®æ³„éœ²
  â€¢ é›†æˆå¤šä¸ªå€™é€‰æ¨¡å‹é™ä½å•ä¸€æ¨¡å‹é£é™©
  â€¢ ä½¿ç”¨Duan smearingçº æ­£å¯¹æ•°å˜æ¢åå·®

{'='*80}
5. å…³é”®å‘ç°æ€»ç»“
{'='*80}

å½±å“å› ç´ æ’åºï¼ˆæŒ‰å½±å“åŠ›å¤§å°ï¼‰:
  1. ç»“æ„æ€§å˜åŒ–: {cp_drop_pct:.1f}% ï¼ˆæœ€å¼ºï¼‰
  2. è¶‹åŠ¿å˜åŒ–: {trend_change_pct:+.1f}%
  3. å‘¨æœ«æ•ˆåº”: {weekend_effect_pct:+.1f}%
  4. èŠ‚å‡æ—¥æ•ˆåº”: {holiday_effect_pct:+.1f}%

ä¸»è¦ç»“è®º:
  â€¢ æ¸¸æˆçƒ­åº¦åœ¨{cp_date.strftime('%Yå¹´%mæœˆ')}å‡ºç°æ˜¾è‘—ä¸‹é™
  â€¢ {'å‘¨æœ«' if weekend_effect_pct > 0 else 'å·¥ä½œæ—¥'}ç©å®¶æ›´æ´»è·ƒ
  â€¢ {'æœ€è¿‘è¶‹åŠ¿ç»§ç»­ä¸‹é™' if trend_change_pct < 0 else 'è¶‹åŠ¿ä¼ç¨³'}
  â€¢ æ¨¡å‹åœ¨å†å²æ•°æ®ä¸Šè¡¨ç°ç¨³å¥ï¼Œå¯ç”¨äºçŸ­æœŸé¢„æµ‹

{'='*80}
"""
    
    # ä¿å­˜æŠ¥å‘Š
    with open(output_dir / 'explanation_report.txt', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("\n" + "="*60)
    print("âœ“ è§£é‡Šæ€§åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: explanation_report.txt")
    print("="*60)
    
    # è¿”å›å…³é”®æŒ‡æ ‡ç”¨äºåç»­å¯è§†åŒ–
    return {
        'cp_drop_pct': cp_drop_pct,
        'weekend_effect_pct': weekend_effect_pct,
        'holiday_effect_pct': holiday_effect_pct,
        'trend_change_pct': trend_change_pct,
        'volatility_change_pct': volatility_change_pct,
        'seasonal_strength': seasonal_strength,
        'overall_std': overall_std,
    }


# ======================
# CV ä¸æ¨¡å‹æ‹Ÿåˆ
# ======================

def rolling_cv(ts: pd.Series, df_dates: pd.Series, order: Tuple, seasonal_order: Tuple,
               n_splits: int = 3, horizon: int = 30) -> List[Dict]:
    """æ»šåŠ¨ CVï¼›æ¯æŠ˜ç‹¬ç«‹å˜ç‚¹â†’æ„å»ºç‰¹å¾â†’æ‹Ÿåˆâ†’é¢„æµ‹ï¼ˆå…¨åœ¨ log ç©ºé—´ï¼‰"""
    results = []
    min_train = 180
    step = max(20, (len(ts) - min_train - horizon) // n_splits)
    
    print(f"    å¼€å§‹æ»šåŠ¨CV (n_splits={n_splits}, horizon={horizon}, min_train={min_train})...")
    
    for i in range(n_splits):
        train_end = min_train + i * step
        test_end = train_end + horizon
        if test_end > len(ts):
            break
        
        print(f"      Fold {i+1}/{n_splits}: train={train_end} days, test={test_end-train_end} days...", end=" ", flush=True)
        
        train, test = ts.iloc[:train_end], ts.iloc[train_end:test_end]
        try:
            cp_idx = detect_changepoint(np.expm1(train))
            
            exog_train = add_features(pd.DataFrame({'Date': df_dates.iloc[:train_end]}), cp_idx, train)
            exog_test  = add_features(pd.DataFrame({'Date': df_dates.iloc[train_end:test_end]}), 0, test)
            exog_test['regime'] = float(exog_train['regime'].iloc[-1])
            exog_test = exog_test[exog_train.columns].fillna(0)
            
            model = SARIMAX(train, exog=exog_train, order=order, seasonal_order=seasonal_order,
                            enforce_stationarity=False, enforce_invertibility=False)
            fitted = model.fit(disp=False, maxiter=50, method='lbfgs')
            
            fc = fitted.get_forecast(steps=len(test), exog=exog_test)
            pred_mean = fc.predicted_mean.values
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨SARIMAXæä¾›çš„é¢„æµ‹åŒºé—´
            pred_summary = fc.summary_frame(alpha=0.10)  # ğŸ”§ ç»Ÿä¸€ï¼š90% CI
            ci_l = pred_summary['mean_ci_lower'].values
            ci_u = pred_summary['mean_ci_upper'].values
            pred_se = (ci_u - ci_l) / (2 * 1.645)  # ğŸ”§ 90% CI å¯¹åº” 1.645

            test_values = test.values
            errors = test_values - pred_mean

            coverage = float(np.mean((test_values >= ci_l) & (test_values <= ci_u)))
            
            rmse = float(np.sqrt(np.mean(errors**2)))
            mase = compute_mase(test_values, pred_mean, train.values)
            wink = compute_winkler_score(test_values, ci_l, ci_u, alpha=0.10)  # ğŸ”§ ç»Ÿä¸€ï¼š90% CI
            pinb = compute_pinball_loss_point(test_values, pred_mean)['Pin@50']
            
            print(f"âœ“ RMSE={rmse:.4f}, Coverage={coverage*100:.1f}%")
            
            results.append({
                'rmse': rmse,
                'pred_var': float(np.mean(pred_se**2)),  # ğŸ”§ ä½¿ç”¨æ­£ç¡®çš„é¢„æµ‹æ–¹å·®
                'coverage': coverage,
                'mase': mase,
                'pinball': pinb,
                'winkler': wink,
                'errors': errors,
                'train_cp_idx': cp_idx,
                'train_end': train_end,
                'pred_mean': pred_mean,
                'pred_se': pred_se,
                'test_values': test_values,
            })
        except Exception as e:
            print(f"âœ— å¤±è´¥: {str(e)[:80]}")
            continue
    
    return results

def fit_models(ts: pd.Series, df_dates: pd.Series) -> Dict:
    print("\næ‹Ÿåˆå€™é€‰æ¨¡å‹ï¼ˆç‹¬ç«‹å˜ç‚¹ + æ»šåŠ¨CVï¼‰...")
    print(f"  æ•°æ®é•¿åº¦: {len(ts)} å¤©")
    
    candidates = [
        ((1, 1, 1), (1, 0, 1, 7), "SARIMA(1,1,1)x(1,0,1,7)"),
        ((2, 1, 1), (0, 0, 0, 0), "ARIMA(2,1,1)"),
        ((1, 1, 2), (1, 0, 1, 7), "SARIMA(1,1,2)x(1,0,1,7)"),
    ]
    
    results = []
    for idx, (order, seasonal_order, name) in enumerate(candidates, 1):
        print(f"\n  [{idx}/{len(candidates)}] è®­ç»ƒæ¨¡å‹: {name}")
        
        import time
        start_time = time.time()
        
        cv = rolling_cv(ts, df_dates, order, seasonal_order, n_splits=3, horizon=30)
        
        elapsed = time.time() - start_time
        print(f"    âœ“ å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f} ç§’")
        
        if not cv:
            print(f"    âš ï¸  æ‰€æœ‰æŠ˜éƒ½å¤±è´¥ï¼Œè·³è¿‡æ­¤æ¨¡å‹")
            continue
            
        avg = lambda k: np.nanmean([r[k] for r in cv])
        print(f"    CVæŒ‡æ ‡: RMSE={avg('rmse'):6.4f}  Coverage={avg('coverage'):5.1%}  "
              f"MASE={avg('mase'):5.3f}  Winkler={avg('winkler'):6.2f}")
        
        results.append({
            'name': name,
            'order': order,
            'seasonal_order': seasonal_order,
            'cv_rmse': avg('rmse'),
            'cv_coverage': avg('coverage'),
            'cv_mase': avg('mase'),
            'cv_winkler': avg('winkler'),
            'cv_pinball': avg('pinball'),
            'pred_var': avg('pred_var'),
            'cv_changepoints': [r.get('train_cp_idx', 0) for r in cv],
            'cv_changepoints_global': [r.get('train_cp_idx', 0) for r in cv],
            'cv_train_lengths': [r.get('train_end', 0) for r in cv],
            'cv_results': cv,
        })
    
    if not results:
        raise ValueError("æ‰€æœ‰æ¨¡å‹æ‹Ÿåˆå¤±è´¥")
    
    best = min(results, key=lambda x: x['cv_rmse'])
    print(f"\nâœ“ æœ€ä¼˜æ¨¡å‹ï¼š{best['name']} (CV-RMSE: {best['cv_rmse']:.4f})")
    return {'models': results, 'best': best}

def forecast_with_ci(ts: pd.Series, exog: pd.DataFrame, exog_future: pd.DataFrame,
                     model_info: Dict, steps: int, verbose: bool = False) -> Dict:
    """å•æ¨¡å‹é¢„æµ‹ï¼ˆlog ç©ºé—´ï¼‰ï¼Œè¿”å›å‡å€¼/é¢„æµ‹æ ‡å‡†è¯¯/90% CI"""
    model = SARIMAX(ts, exog=exog,
                    order=model_info['order'], seasonal_order=model_info['seasonal_order'],
                    enforce_stationarity=False, enforce_invertibility=False)
    fitted = model.fit(disp=False, maxiter=50, method='lbfgs')
    fc = fitted.get_forecast(steps=steps, exog=exog_future)
    
    pred_mean = fc.predicted_mean.values
    
    # ğŸ”§ ç»Ÿä¸€ä½¿ç”¨ 90% CI (alpha=0.10)
    pred_summary = fc.summary_frame(alpha=0.10)  # 90% CI
    ci_lower = pred_summary['mean_ci_lower'].values
    ci_upper = pred_summary['mean_ci_upper'].values
    
    # ä»ç½®ä¿¡åŒºé—´åæ¨æ ‡å‡†è¯¯ï¼ˆ90% CI å¯¹åº” 1.645 å€æ ‡å‡†è¯¯ï¼‰
    pred_se = (ci_upper - ci_lower) / (2 * 1.645)
    
    if verbose:
        print(f"  [{model_info['name']}] mean(pred_se)={np.mean(pred_se):.4f}")
    
    return {
        'forecast': pred_mean,
        'se': pred_se,
        'ci_lower': ci_lower,
        'ci_upper': ci_upper,
    }


def ensemble_forecast(ts: pd.Series, exog: pd.DataFrame, exog_future: pd.DataFrame,
                      model_results: List[Dict], steps: int, verbose: bool = False) -> Dict:
    """åˆå¥ï¼ˆlog ç©ºé—´ï¼‰ï¼šIVW åŠ æƒå‡å€¼ + æ–¹å·®åˆå¹¶ï¼Œè¾“å‡º 90% CI"""
    means_log, ses_log, inv_vars = [], [], []
    
    for m in model_results:
        r = forecast_with_ci(ts, exog, exog_future, m, steps, verbose=False)
        means_log.append(r['forecast'])
        ses_log.append(r['se'])
        # IVWæƒé‡åŸºäºCVçš„é¢„æµ‹æ–¹å·®
        inv_vars.append(1.0 / (m['pred_var'] + 1e-6))

    mu_log, se_log, w = combine_mean_se(means_log, ses_log, np.array(inv_vars))

    if verbose:
        print("\nç”Ÿæˆé›†æˆé¢„æµ‹...")
        print("æƒé‡åˆ†é…ï¼ˆInverse Variance Weightingï¼‰ï¼š")
        for wi, mi in zip(w, model_results):
            print(f"  {mi['name']:<30} {wi*100:5.1f}%")
        print(f"\né¢„æµ‹åŒºé—´ç»Ÿè®¡ï¼ˆlogç©ºé—´ï¼Œ90% CIï¼‰ï¼š")
        print(f"  å¹³å‡é¢„æµ‹æ ‡å‡†è¯¯ï¼š{np.mean(se_log):.4f}")
        print(f"  æ ‡å‡†è¯¯èŒƒå›´ï¼š[{np.min(se_log):.4f}, {np.max(se_log):.4f}]")

    lo_log = mu_log - 1.645 * se_log  # ğŸ”§ ç»Ÿä¸€ï¼š90% CI
    hi_log = mu_log + 1.645 * se_log

    return {
        'forecast': mu_log, 
        'se': se_log,
        'ci_lower': lo_log, 
        'ci_upper': hi_log, 
        'weights': w
    }


def walk_forward_validation(ts: pd.Series, df_dates: pd.Series, 
                            model_results: List[Dict], 
                            horizons: List[int] = [30, 60], 
                            verbose: bool = False) -> Dict:
    """Walk-forward éªŒè¯ï¼šæµ‹è¯•é›†æˆé¢„æµ‹åœ¨ä¸åŒé¢„æµ‹çª—å£çš„è¦†ç›–ç‡ï¼ˆlog ç©ºé—´ï¼‰"""
    results = {}
    min_train = 180  # æœ€å°è®­ç»ƒé›†é•¿åº¦
    
    for h in horizons:
        # ğŸ”§ ä¿®å¤ï¼šåªéœ€è¦èƒ½è‡³å°‘æµ‹è¯•ä¸€æ¬¡å³å¯
        if len(ts) < min_train + h:
            if verbose:
                print(f"  [h={h}] è·³è¿‡ï¼šæ•°æ®ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘ {min_train + h} å¤©ï¼Œå½“å‰ {len(ts)} å¤©ï¼‰")
            continue
            
        coverages = []
        test_start = min_train
        
        while test_start + h <= len(ts):
            train = ts.iloc[:test_start]
            test = ts.iloc[test_start:test_start + h]
            
            if len(test) < h:
                break
            
            try:
                # æ„å»ºç‰¹å¾
                cp_idx = detect_changepoint(np.expm1(train))
                exog_train = add_features(pd.DataFrame({'Date': df_dates.iloc[:test_start]}), cp_idx, train)
                exog_test = add_features(pd.DataFrame({'Date': df_dates.iloc[test_start:test_start+h]}), 0, test)
                exog_test['regime'] = float(exog_train['regime'].iloc[-1])
                exog_test = exog_test[exog_train.columns].fillna(0)
                
                # é›†æˆé¢„æµ‹ï¼ˆlog ç©ºé—´ï¼‰
                ens = ensemble_forecast(train, exog_train, exog_test, model_results, steps=h, verbose=False)
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šè½¬æ¢åˆ°åŸå§‹å°ºåº¦å†è®¡ç®—è¦†ç›–ç‡
                # è·å–æ®‹å·®ç”¨äº smearing
                train_resid = []
                for m in model_results:
                    if m.get('cv_results'):
                        for fold in m['cv_results']:
                            if 'errors' in fold:
                                train_resid.extend(fold['errors'])
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šåœ¨ log ç©ºé—´è®¡ç®—è¦†ç›–ç‡ï¼Œé¿å… Jensen ä¸ç­‰å¼é—®é¢˜
                # log ç©ºé—´çš„ CI æ˜¯å¯¹ç§°çš„ï¼Œè½¬åˆ°åŸå§‹å°ºåº¦ä¼šä¸å¯¹ç§°
                ci_l_log = ens['ci_lower']
                ci_u_log = ens['ci_upper']
                test_vals_log = test.values
                
                # åœ¨ log ç©ºé—´è®¡ç®—è¦†ç›–ç‡ï¼ˆæ­£ç¡®åšæ³•ï¼‰
                coverage = np.mean((test_vals_log >= ci_l_log) & (test_vals_log <= ci_u_log))
                coverages.append(coverage)
                
                if verbose:
                    print(f"    [h={h}, start={test_start}] Coverage={coverage*100:.1f}%")
                
            except Exception as e:
                if verbose:
                    print(f"    [h={h}, start={test_start}] å¤±è´¥: {str(e)[:50]}")
                pass
            
            test_start += 30  # æ¯æ¬¡å‰è¿›30å¤©
        
        if coverages:
            results[h] = {
                'coverage': np.mean(coverages),
                'n_tests': len(coverages)
            }
            if verbose or True:  # æ€»æ˜¯æ‰“å°è¯¦ç»†ç»“æœ
                print(f"  [h={h}å¤©] å¹³å‡è¦†ç›–ç‡: {np.mean(coverages)*100:.1f}% (n={len(coverages)})")
    
    return results


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", type=Path, default=Path("/Users/1m/Desktop/å¤§ä¸‰ä¸Š/å¤§æ•°æ®/æœŸæœ«/data.xlsx"))
    parser.add_argument("--output-dir", type=Path, default=Path(__file__).parent / "results")
    parser.add_argument("--history-window", type=int, default=None,
                        help="ä¸»è®­ç»ƒçª—å£é•¿åº¦ï¼ˆNone=å…¨å†å²ï¼‰")
    args = parser.parse_args()
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    print("="*60)
    print("Q1ï¼šæ—¶é—´åºåˆ—é›†æˆé¢„æµ‹ï¼ˆç²¾ç®€ç¨³å¥ç‰ˆï¼‰")
    print("="*60)
    
    # 1) æ•°æ®
    print("\n[1/8] åŠ è½½æ•°æ®...")
    df = load_data(args.input)
    ts_raw_full = df.set_index('Date')['Number of  reported results'].sort_index()
    
    if args.history_window:
        ts_raw = ts_raw_full.iloc[-args.history_window:]
        df = df[df['Date'].isin(ts_raw.index)].copy()
        print(f"  âœ“ ä½¿ç”¨æœ€è¿‘ {args.history_window} å¤©æ•°æ® ({ts_raw.index[0].date()} è‡³ {ts_raw.index[-1].date()})")
    else:
        ts_raw = ts_raw_full
        print(f"  âœ“ ä½¿ç”¨å…¨éƒ¨å†å²æ•°æ® ({len(ts_raw)} å¤©)")
    
    ts_log = np.log1p(ts_raw)
    
    # 2) å˜ç‚¹æ‘˜è¦
    print("\n[2/8] æ£€æµ‹å˜ç‚¹...")
    cp_idx = detect_changepoint(ts_raw)
    cp_date = ts_raw.index[cp_idx]
    pre_mean = ts_raw.iloc[:cp_idx].mean()
    post_mean = ts_raw.iloc[cp_idx:].mean()
    print(f"  âœ“ å˜ç‚¹: {cp_date.date()} (ç¬¬{cp_idx}å¤©)")
    print(f"    å˜ç‚¹å‰å‡å€¼: {pre_mean:,.0f} äºº/å¤©")
    print(f"    å˜ç‚¹åå‡å€¼: {post_mean:,.0f} äºº/å¤©")
    print(f"    ä¸‹é™å¹…åº¦: {(post_mean-pre_mean)/pre_mean*100:.1f}%")

    # 3) å¤–ç”Ÿå˜é‡
    print("\n[3/8] æ„å»ºç‰¹å¾...")
    exog = add_features(df, cp_idx, ts_log)
    print(f"  âœ“ ç”Ÿæˆ {len(exog.columns)} ä¸ªå¤–ç”Ÿå˜é‡: {list(exog.columns)}")

    # 4) æ‹Ÿåˆå€™é€‰ + CV
    print("\n[4/8] æ‹Ÿåˆå¹¶äº¤å‰éªŒè¯å€™é€‰æ¨¡å‹...")
    print("  (è¿™ä¸€æ­¥å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...)")
    import time
    total_start = time.time()
    models = fit_models(ts_log, df['Date'])
    total_elapsed = time.time() - total_start
    print(f"\n  âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæ€»è€—æ—¶ {total_elapsed:.1f} ç§’")

    # 4.5) ç”Ÿæˆè§£é‡Šæ€§åˆ†ææŠ¥å‘Š
    print("\n[5/8] ç”Ÿæˆè§£é‡Šæ€§åˆ†ææŠ¥å‘Š...")
    explanation_stats = generate_explanation_report(ts_raw, exog, models, cp_idx, args.output_dir)

    # 5) æ®‹å·®è¯Šæ–­
    print("\n[6/8] ç”Ÿæˆè¯Šæ–­å›¾è¡¨...")
    best = models['best']
    cv_for_diag = rolling_cv(ts_log, df['Date'], best['order'], best['seasonal_order'], n_splits=3, horizon=30)
    diag_stats = diagnostic_plots(ts_log, exog, best, cv_for_diag, args.output_dir)
    resid_stable = diag_stats.get('residuals_stable', pd.Series(dtype=float)).dropna().values

    # 6) Walk-Forward
    print("\n[7/8] Walk-Forward éªŒè¯...")
    wf_results = walk_forward_validation(ts_log, df['Date'], models['models'], horizons=[30, 60], verbose=False)
    for h, stats in wf_results.items():
        print(f"  h={h}å¤©: è¦†ç›–ç‡={stats['coverage']*100:.1f}% (n={stats['n_tests']})")  # ğŸ”§ ä¿®å¤ï¼šä¹˜ä»¥100
    
    # 7) æœªæ¥é¢„æµ‹
    print("\n[8/8] ç”Ÿæˆæœªæ¥é¢„æµ‹...")
    last_date = ts_log.index[-1]
    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=60, freq='D')
    exog_future = add_features(pd.DataFrame({'Date': future_dates}), 0, pd.Series(index=future_dates))
    exog_future['regime'] = float(exog['regime'].iloc[-1])
    
    # 8) åˆå¥
    ens_log = ensemble_forecast(ts_log, exog, exog_future, models['models'], steps=60, verbose=True)
    
    # 9) å›åŸå°ºåº¦ï¼ˆå·²åœ¨ ensemble_forecast ä¸­è®¡ç®—äº† 90% CIï¼‰
    mu_log, se_log = ens_log['forecast'], ens_log['se']
    lo_log, hi_log = ens_log['ci_lower'], ens_log['ci_upper']  # ğŸ”§ ç›´æ¥ä½¿ç”¨ 90% CI

    y_pred = duan_smearing(mu_log, resid_stable)
    y_lo   = np.expm1(lo_log)
    y_hi   = np.expm1(hi_log)
    
    # 10) å¯è§†åŒ–ä¸æŠ¥å‘Š
    print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    generate_diagnostic_report(best, cv_for_diag, diag_stats, wf_results, args.output_dir)
    plot_weekday_effects(ts_raw, cp_idx, args.output_dir)
    plot_changepoint_summary(ts_raw, cp_idx, args.output_dir)
    plot_factor_importance(explanation_stats, args.output_dir)
    
    # 11) ä¿å­˜ç»“æœ
    with open(args.output_dir / "ensemble_result.pkl", "wb") as f:
        pickle.dump({'forecast': y_pred, 'ci_lower': y_lo, 'ci_upper': y_hi, 'weights': ens_log['weights']}, f)
    
    # 12) è¾“å‡ºç›®æ ‡æ—¥
    target_idx = (pd.to_datetime('2023-03-01') - future_dates[0]).days
    if 0 <= target_idx < 60:
        pred = y_pred[target_idx]; lo = y_lo[target_idx]; hi = y_hi[target_idx]
        half_w = (hi - lo)/2
        recent_std = ts_raw.iloc[-30:].std()
        print("\n" + "="*60)
        print("ã€2023-03-01 é¢„æµ‹ã€‘")
        print("="*60)
        print(f"ç‚¹é¢„æµ‹ï¼š{pred:,.0f} äºº")
        print(f"90% CIï¼š[{lo:,.0f}, {hi:,.0f}]")  # ğŸ”§ ç»Ÿä¸€ä½¿ç”¨ 90% CI
        print(f"åŒºé—´å®½åº¦ï¼šÂ±{half_w:,.0f} äººï¼ˆâ‰ˆ{half_w/recent_std:.1f}Ã— æœ€è¿‘30å¤©stdï¼‰")
        
    print("\n" + "="*60)
    print(f"âœ“ æ‰€æœ‰è¾“å‡ºå·²ä¿å­˜è‡³: {args.output_dir}")
    print("="*60)
    print("  æ–‡æœ¬æŠ¥å‘Š:")
    print("    - explanation_report.txt")
    print("    - diagnostic_report.txt")
    print("  å¯è§†åŒ–å›¾è¡¨:")
    print("    - 1_weekday_effects.png")
    print("    - 2_changepoint.png")
    print("    - 3_diagnostics.png")
    print("    - 4_factor_importance.png")
    print("  æ¨¡å‹æ–‡ä»¶:")
    print("    - ensemble_result.pkl")
    print("\nä¸‹ä¸€æ­¥:")
    print("  è¿è¡Œ python model_comparison.py --input ../data.xlsx")
    print("  å¯¹æ¯” Ensemble vs Prophet vs Chronos")
    print("="*60)

if __name__ == "__main__":
    main()
