
================================================================================
MoE (Mixture of Experts) + MLP + Softmax 分布预测总结报告
================================================================================

一、模型配置
-----------
专家数量 (num_experts): 2
每个专家的隐藏层大小 (hidden_size): 64
Top-K 路由 (k): 1
辅助损失系数 (aux_coef): 0.001
学习率 (lr): 0.005
权重衰减 (weight_decay): 0.0001
早停耐心 (patience): 50

二、训练结果
-----------
最佳验证轮次: 63
最佳验证损失: 1.583633
总训练轮次: 113

三、测试集评估指标
-----------------
MAE (平均绝对误差): 0.031540
  - 含义: 7个桶的概率平均误差约 3.15%

KL散度 (KL Divergence): 0.223923
  - 含义: 预测分布与真实分布的信息差异 (越小越好)

JS散度 (Jensen-Shannon): 0.109338
  - 含义: 对称的分布差异度量 (0~1, 越小越好)

余弦相似度 (Cosine Similarity): 0.974266
  - 含义: 分布向量的相似程度 (越接近1越好)

R² (决定系数): 0.218974
  - 含义: 模型解释方差的比例 (越接近1越好)

最大误差 (Max Error): 0.273982
  - 含义: 单个桶上的最大预测偏差

四、专家使用率分析
-----------------
Expert 0: 使用率=46.3%, 平均门控权重=0.4630
Expert 1: 使用率=53.7%, 平均门控权重=0.5370

五、模型解读
-----------
1. MoE 模型通过门控网络将不同样本路由到不同的专家网络
2. 每个样本由 top-1 个专家共同处理，加权组合输出
3. 辅助损失确保各专家被均衡使用，避免"专家塌陷"

六、输出文件
-----------
预测结果: moe_output/moe_softmax_pred_output.csv
训练曲线: moe_output/moe_training_history.png
分布对比: moe_output/moe_distribution_comparison.png
误差分析: moe_output/moe_error_analysis.png
专家使用率: moe_output/moe_expert_usage.png
辅助损失曲线: moe_output/moe_aux_loss.png
综合汇总: moe_output/moe_comprehensive_summary.png
JSON报告: moe_output/moe_report.json

================================================================================
报告结束
================================================================================
